# WAFL-MLP

Wireless Ad Hoc Federated Learning (WAFL). This project provides the code for "Wireless Ad Hoc Federated Learning: A Fully Distributed Cooperative Machine Learning". MLP stands for multi-layer perceptron -- one of the most basic neural networks for beginners.

## Introduction
Privacy-sensitive data is stored in autonomous vehicles, smart devices, or sensor nodes that can move around and can make opportunistic contact with each other. Geographical locations, private photos, healthcare signals, and power consumption of homes are examples. Federation among such nodes was mainly discussed in the context of federated learning (FL) in many works with a centralized parameter server. However, because of multi-vendor issues, they do not want to rely on a specific server operated by a third party for this purpose but want to directly interact with each other in an ad hoc manner only when they have in contact just as cooperative intelligent transport systems do.

Wireless ad hoc federated learning (WAFL) allows collaborative learning with device-to-device communications organized by the devices physically nearby. Here, each node has a wireless interface and can communicate with each other when they are within the radio range. The nodes are expected to move with people, vehicles, or robots, producing opportunistic contacts with each other.

## Architecture
<img src="./assets/wafl_overview.png" width="75%">

We start the discussion from the most basic peer-to-peer case as shown in the figure. In this scenario, each node trains a model individually with the local data it has. Here, node 1 can be a Toyota car, and node 2 can be a Mitsubishi car. When a node encounters another node, they exchange their local models with each other through the ad hoc communication channel. Then, the node aggregates the models into a new model, which is expected to be more general compared to the locally trained models. With an adjustment process of the new model with the local training data, they repeat this process during they are in contact. Please note that there is no third-party server operated for the federation among multi-vendor devices.

As WAFL does not collect data from users, the distributions of the data on individual nodes are not the same; e.g., a user has a larger portion of photos of hamburgers, but another user has a larger portion of dumplings based on their preferences or circumstances. This is a well-known problem of conventional federated learning as ``user data is not independent and identically distributed (Non-IID)''. The challenge is to develop a general model which does not over-fit into specific user data on the fully distributed, or partially-connected environment. 

## Model Aggregation via Ad Hoc Contacts

<img src="./assets/wafl_contact_model_aggregation.png">

Model exchange and aggregation with encountered smart devices in wireless ad hoc federated learning (WAFL). The nodes exchange and aggregate their models among the nodes encountered in an ad hoc manner. The initial models are trained too specific to their local Non-IID data, but in the long run, many contacts allow the mixture of locally trained models, making them more generalized.

## Mobility Pattern Prepared for Simulation

### RWP0500

<img src="./assets/rwp0500.gif">

Random Waypoint Mobility (RWP) is often used in wireless ad hoc network simulations. People walk around the given area with posing some duration at temporal locations. This mobility model allows a device to contact with any other devices in the area. This mobility model greatly fits to the movement of people in a shopping mall or in an amusement park. Depending on the size of the people's movable areas, we generated three categories of mobility patterns: i.e., rwp0500, rwp1000, and rwp2000. Here, the name indicates the size of the movable area, for example, rwp0500 was generated by simulating node movements in 500 [m] x 500 [m] square. We assumed 100 [m] for radio range and 10 [epoch] for posing time. We have chosen the travel speed uniformly randomly from the interval of [3.0, 7.0] [m/epoch].

### Contact Pattern for Simulation
<img src="./assets/contact_pattern.png">

Other mobility/contact patterns prepared for simulation includes static_line, static_tree, static_ringstar, static_dense, and community structured environment (CSE). In static ones, devices do not move and they always have stable contacts with neighbor nodes. CSE is based on the idea that there are several communities in the real society to which people belong, and that they usually meet the people who belong to the same community. CSE assumes that a device belongs to several communities and moves from one community to another. Here, the community is a physical rendezvous point. If two devices are in the same community at the same time, they can communicate with each other.

The figure shows topology examples of device-to-device contacts. The solid lines indicate static connections -- the devicess are always connected and can communicate all the time. The dashed lines indicate opportunistic connections -- the devices will have temporal connections and can communicate while the connections are alive.

Althrough we have prepared these contact patterns, you will see there are no such a difference in learning accuracy. It means WAFL has no critical dependencies in contact patterns or mobility (topology). Thus, in practical experiment, rwp0500 or static_line can be used for understanding the basic characteristics.

## How to Run

### Module installation

This code has been tested and verified to work with Python 3.11.4 and CUDA 11.4.
The specific versions of key dependencies used in our test environment are listed in the `requirements.txt` file.

However, please note that you may need to adjust the versions, especially for `torch` and `torchvision`, to match your specific environment and CUDA version.

After ensuring versions of required dependencies, install them by following commands:

```Linux
pip install -r requirements.txt
```

If you encounter any issues, you may need to modify the versions in `requirements.txt` to suit your specific setup. In particular, ensure that the `torch` and `torchvision` versions are compatible with your CUDA installation if you're using GPU acceleration.




## Confusion Matrix

<img src="./assets/confusion_matrix.png">


## References 
\[1\] Hideya Ochiai, Yuwei Sun, Qingzhe Jin, Nattanon Wongwiwatchai, Hiroshi Esaki, "Wireless Ad Hoc Federated Learning: A Fully Distributed Cooperative Machine Learning" in May 2022 (https://arxiv.org/abs/2205.11779). 
